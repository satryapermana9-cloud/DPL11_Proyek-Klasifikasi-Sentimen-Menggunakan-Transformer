{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# ==============================\n",
        "# KOMPONEN BAHASA\n",
        "# ==============================\n",
        "\n",
        "subjek = [\n",
        "    \"Aplikasi ini\", \"Aplikasi mobile ini\", \"Sistem aplikasi\",\n",
        "    \"Platform digital ini\", \"Layanan aplikasi\", \"Aplikasi yang digunakan\"\n",
        "]\n",
        "\n",
        "objek = [\n",
        "    \"pengguna\", \"pengguna baru\", \"pengguna lama\",\n",
        "    \"saya\", \"tim kerja\", \"pengguna aplikasi\"\n",
        "]\n",
        "\n",
        "konteks = [\n",
        "    \"dalam penggunaan sehari-hari\",\n",
        "    \"pada versi terbaru\",\n",
        "    \"saat digunakan secara rutin\",\n",
        "    \"di perangkat Android\",\n",
        "    \"di perangkat iOS\",\n",
        "    \"secara umum\",\n",
        "    \"dalam kondisi normal\",\n",
        "    \"\"\n",
        "]\n",
        "\n",
        "positif = [\n",
        "    \"sangat membantu\",\n",
        "    \"berjalan dengan sangat baik\",\n",
        "    \"memberikan pengalaman yang memuaskan\",\n",
        "    \"memudahkan pekerjaan\",\n",
        "    \"memiliki performa yang stabil\",\n",
        "    \"sangat berguna\",\n",
        "    \"mudah digunakan\",\n",
        "    \"responsif saat dijalankan\",\n",
        "    \"meningkatkan produktivitas\",\n",
        "    \"berfungsi dengan optimal\"\n",
        "]\n",
        "\n",
        "negatif = [\n",
        "    \"sering mengalami error\",\n",
        "    \"berjalan sangat lambat\",\n",
        "    \"sulit untuk digunakan\",\n",
        "    \"sering mengalami gangguan\",\n",
        "    \"tidak berjalan dengan baik\",\n",
        "    \"mengecewakan pengguna\",\n",
        "    \"tidak responsif\",\n",
        "    \"memiliki banyak bug\",\n",
        "    \"menghambat pekerjaan\",\n",
        "    \"bermasalah saat dijalankan\"\n",
        "]\n",
        "\n",
        "netral = [\n",
        "    \"dapat digunakan\",\n",
        "    \"tersedia untuk pengguna\",\n",
        "    \"memiliki beberapa fitur\",\n",
        "    \"menyediakan menu utama\",\n",
        "    \"digunakan untuk kebutuhan tertentu\",\n",
        "    \"berjalan sesuai fungsi dasar\",\n",
        "    \"tersedia di berbagai platform\",\n",
        "    \"memiliki tampilan standar\",\n",
        "    \"digunakan oleh banyak pengguna\",\n",
        "    \"menyediakan layanan digital\"\n",
        "]\n",
        "\n",
        "# ==============================\n",
        "# FUNGSI PEMBENTUK KALIMAT\n",
        "# ==============================\n",
        "\n",
        "def buat_kalimat(predikat_list):\n",
        "    return f\"{random.choice(subjek)} {random.choice(predikat_list)} bagi {random.choice(objek)} {random.choice(konteks)}\".strip()\n",
        "\n",
        "# ==============================\n",
        "# DATA LATIH (300)\n",
        "# ==============================\n",
        "\n",
        "train_data = []\n",
        "\n",
        "for _ in range(100):\n",
        "    train_data.append([buat_kalimat(positif), \"Positif\"])\n",
        "    train_data.append([buat_kalimat(negatif), \"Negatif\"])\n",
        "    train_data.append([buat_kalimat(netral),  \"Netral\"])\n",
        "\n",
        "df_train = pd.DataFrame(train_data, columns=[\"kalimat\", \"label\"])\n",
        "df_train = df_train.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "df_train.to_csv(\"data_latih_300.csv\", index=False)\n",
        "\n",
        "# ==============================\n",
        "# DATA UJI (400)\n",
        "# ==============================\n",
        "\n",
        "test_sentences = []\n",
        "semua_predikat = positif + negatif + netral\n",
        "\n",
        "for _ in range(400):\n",
        "    kalimat = f\"{random.choice(subjek)} {random.choice(semua_predikat)} bagi {random.choice(objek)} {random.choice(konteks)}\"\n",
        "    test_sentences.append(kalimat.strip())\n",
        "\n",
        "df_test = pd.DataFrame(test_sentences, columns=[\"kalimat\"])\n",
        "df_test.to_csv(\"data_uji_400.csv\", index=False)\n",
        "\n",
        "# ==============================\n",
        "# CEK HASIL\n",
        "# ==============================\n",
        "\n",
        "print(\"âœ… Dataset berhasil dibuat\\n\")\n",
        "print(\"Data Latih:\")\n",
        "print(df_train['label'].value_counts(), \"\\n\")\n",
        "print(\"Contoh Data Latih:\")\n",
        "print(df_train.head(5), \"\\n\")\n",
        "\n",
        "print(\"Data Uji:\")\n",
        "print(\"Jumlah:\", len(df_test))\n",
        "print(\"Contoh Data Uji:\")\n",
        "print(df_test.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JruFdAw4M2dZ",
        "outputId": "93acda6b-b406-444c-9606-386664ccec24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset berhasil dibuat\n",
            "\n",
            "Data Latih:\n",
            "label\n",
            "Netral     100\n",
            "Positif    100\n",
            "Negatif    100\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "Contoh Data Latih:\n",
            "                                             kalimat    label\n",
            "0  Sistem aplikasi memiliki beberapa fitur bagi p...   Netral\n",
            "1  Aplikasi ini memiliki tampilan standar bagi pe...   Netral\n",
            "2  Layanan aplikasi tersedia di berbagai platform...   Netral\n",
            "3  Sistem aplikasi berjalan dengan sangat baik ba...  Positif\n",
            "4  Aplikasi ini tersedia untuk pengguna bagi peng...   Netral \n",
            "\n",
            "Data Uji:\n",
            "Jumlah: 400\n",
            "Contoh Data Uji:\n",
            "                                             kalimat\n",
            "0  Aplikasi yang digunakan memiliki tampilan stan...\n",
            "1  Platform digital ini memberikan pengalaman yan...\n",
            "2  Aplikasi yang digunakan menyediakan menu utama...\n",
            "3  Layanan aplikasi berjalan sangat lambat bagi p...\n",
            "4  Aplikasi ini meningkatkan produktivitas bagi t...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKHiWaADztjM",
        "outputId": "71aeac62-f3b0-463e-d168-816e4c4338ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 | Loss: 1.3930\n",
            "Epoch 2/30 | Loss: 1.1298\n",
            "Epoch 3/30 | Loss: 1.0842\n",
            "Epoch 4/30 | Loss: 1.0657\n",
            "Epoch 5/30 | Loss: 0.9943\n",
            "Epoch 6/30 | Loss: 0.7450\n",
            "Epoch 7/30 | Loss: 0.4195\n",
            "Epoch 8/30 | Loss: 0.2376\n",
            "Epoch 9/30 | Loss: 0.1226\n",
            "Epoch 10/30 | Loss: 0.0572\n",
            "Epoch 11/30 | Loss: 0.0226\n",
            "Epoch 12/30 | Loss: 0.0103\n",
            "Epoch 13/30 | Loss: 0.0071\n",
            "Epoch 14/30 | Loss: 0.0059\n",
            "Epoch 15/30 | Loss: 0.0054\n",
            "Epoch 16/30 | Loss: 0.0049\n",
            "Epoch 17/30 | Loss: 0.0047\n",
            "Epoch 18/30 | Loss: 0.0042\n",
            "Epoch 19/30 | Loss: 0.0040\n",
            "Epoch 20/30 | Loss: 0.0037\n",
            "Epoch 21/30 | Loss: 0.0035\n",
            "Epoch 22/30 | Loss: 0.0034\n",
            "Epoch 23/30 | Loss: 0.0033\n",
            "Epoch 24/30 | Loss: 0.0031\n",
            "Epoch 25/30 | Loss: 0.0030\n",
            "Epoch 26/30 | Loss: 0.0027\n",
            "Epoch 27/30 | Loss: 0.0026\n",
            "Epoch 28/30 | Loss: 0.0025\n",
            "Epoch 29/30 | Loss: 0.0024\n",
            "Epoch 30/30 | Loss: 0.0023\n",
            "\n",
            "âœ… PROSES SELESAI\n",
            "ðŸ“„ File dihasilkan:\n",
            "- hasil_pelabelan_400_kalimat.csv\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# TRANSFORMER SENTIMENT CLASSIFICATION\n",
        "# FINAL PIPELINE - COLAB READY\n",
        "# ==========================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# 1. LOAD DATA\n",
        "# ==========================================\n",
        "\n",
        "df_train = pd.read_csv(\"data_latih_300.csv\")\n",
        "df_test  = pd.read_csv(\"data_uji_400.csv\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. PREPROCESSING\n",
        "# ==========================================\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "df_train['cleaned'] = df_train['kalimat'].apply(clean_text)\n",
        "df_test['cleaned']  = df_test['kalimat'].apply(clean_text)\n",
        "\n",
        "# ==========================================\n",
        "# 3. TOKENIZATION (VOCAB FROM TRAINING DATA)\n",
        "# ==========================================\n",
        "\n",
        "all_words = ' '.join(df_train['cleaned']).split()\n",
        "vocab = sorted(set(all_words))\n",
        "word_to_idx = {w: i + 1 for i, w in enumerate(vocab)}\n",
        "\n",
        "MAX_LEN = 15\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = [word_to_idx.get(w, 0) for w in text.split()]\n",
        "    return (tokens + [0] * MAX_LEN)[:MAX_LEN]\n",
        "\n",
        "X_train = np.array(df_train['cleaned'].apply(tokenize).tolist())\n",
        "X_test  = np.array(df_test['cleaned'].apply(tokenize).tolist())\n",
        "\n",
        "# ==========================================\n",
        "# 4. LABEL ENCODING\n",
        "# ==========================================\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(df_train['label'])\n",
        "\n",
        "# ==========================================\n",
        "# 5. DATASET & DATALOADER\n",
        "# ==========================================\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.X = torch.LongTensor(X)\n",
        "        self.y = torch.LongTensor(y) if y is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.X[idx], self.y[idx]) if self.y is not None else self.X[idx]\n",
        "\n",
        "train_loader = DataLoader(TextDataset(X_train, y_train), batch_size=32, shuffle=True)\n",
        "\n",
        "# ==========================================\n",
        "# 6. TRANSFORMER MODEL\n",
        "# ==========================================\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, MAX_LEN, embed_dim))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim, nhead=num_heads, batch_first=True\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.pos_embed\n",
        "        x = self.encoder(x)\n",
        "        x = x.mean(dim=1)\n",
        "        return self.fc(x)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = TransformerClassifier(\n",
        "    vocab_size=len(word_to_idx) + 1,\n",
        "    embed_dim=64,\n",
        "    num_heads=8,\n",
        "    num_layers=2,\n",
        "    num_classes=len(le.classes_)\n",
        ").to(device)\n",
        "\n",
        "# ==========================================\n",
        "# 7. TRAINING\n",
        "# ==========================================\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "EPOCHS = 30\n",
        "model.train()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(model(Xb), yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 8. INFERENCE (400 TEST SENTENCES)\n",
        "# ==========================================\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds = torch.argmax(\n",
        "        model(torch.LongTensor(X_test).to(device)),\n",
        "        dim=1\n",
        "    )\n",
        "\n",
        "df_test['hasil_label_transformer'] = le.inverse_transform(preds.cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# 9. SAVE OUTPUT\n",
        "# ==========================================\n",
        "\n",
        "df_test.to_csv(\"hasil_pelabelan_400_kalimat.csv\", index=False)\n",
        "\n",
        "print(\"\\nâœ… PROSES SELESAI\")\n",
        "print(\"ðŸ“„ File dihasilkan:\")\n",
        "print(\"- hasil_pelabelan_400_kalimat.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"hasil_pelabelan_400_kalimat.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "yZXdIhKFOz-z",
        "outputId": "fdcfa4c9-a61e-482d-e490-65be82efd490"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_269543ad-c4b7-40f7-af10-cf1f7d1a6ba4\", \"hasil_pelabelan_400_kalimat.csv\", 65061)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('data_latih_300.csv')\n",
        "files.download('data_uji_400.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5BuBO3aCQ2wE",
        "outputId": "65c6bb2b-8cbb-40bf-cc47-1fa7fe9cd296"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fbea2a04-5fa9-487d-b3a7-d8f1488dc1e0\", \"data_latih_300.csv\", 25739)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6464ad37-2704-4ec4-a44f-b36f62ab264c\", \"data_uji_400.csv\", 31005)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}